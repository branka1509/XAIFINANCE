## Overview

This tab allows you to obtain the classic global and local explanations as to how the different ML or DL models arrived at their respective predictions. Specifically, you can first choose the model that you would like explained and at a second instance, you can choose to explore one of the following explanations:

-   **(Global) Variable Importance Plot.** This visualization presents the relative importance of the different input features included in the model specification. Within our VA, this plot is available for all ML models apart from the ensemble models. The relative influence of each feature is determined by examining the tree building process and checking whether the variable in question was selected in a splitting criteria and if as a result, the squared error of all trees decreased.
-   **(Global) Variable Importance Plot.** This visualization presents the relative importance of the different input features included in the model specification. Within our VA, this plot is available for all ML models apart from the ensemble models. The relative influence of each feature is determined by examining the tree building process and checking whether the variable in question was selected in a splitting criteria and if as a result, the squared error of all trees decreased.
-   **(Global) SHAP Summary.** SHAP, short for SHapleyAdditive exPlanations, presents a unified framework for interpreting predictions and it is based on the game theoretically optimal Shapley values. According to the paper by [32], for each prediction instance, SHAP assigns an importance score for each feature included in the model's specification. Its novel components include: (i) the identification of a new class of additive feature importance measures, and (ii) theoretical results showing there is a unique solution in this class with a set of desirable properties. The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction.
-   **(Local) SHAP row-specific explanations:** This method shows contribution of features for a given instance. The sum of the feature contributions and the bias term is equal to the raw prediction of the model, i.e., prediction before applying inverse link function. H2O implements TreeSHAP which when the features are correlated, can increase contribution of a feature that had no influence on the prediction.

Each of the selected explanations are accompanied by detailed interpretations of the visualizations displayed and a general comment of whether the model is in line with financial logic.
