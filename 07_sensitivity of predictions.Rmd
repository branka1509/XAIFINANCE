---
title: "Sensitivity: toy data"
output: html_notebook
---


This notebook contains the project's outputs with respect to WP4, specifically looking at stability of predictions.

In the following script we run a sensitivity analysis of the predictions obtained from the autoML model. Specifically, our goal is to understand how uncertain an output is within a certain mathematical model by considering how changes in the inputs affect the obtained predicted probabilities. 

The process of recalculating outputs under the alternative assumptions so to determine the impact of the variable can be summarized in the following steps:
* using the original data, an autoML object is fitted (original automl model = toy_aml)

1) adding noise to a certain variable 
* we choose a variable to which we add varying level of noise. The noise is determined in the following manner: let's take z = max(x) - min(x), the level of noise added, x + runif(n, -a, +a), is defined though the changing of a, where n is the number of observations in x. a, on the other hand, can be defined as: 
  ** (i) a = z/50; 
  ** (ii) = (1.1; 1.5; 2.5; 5) x z/50, 
  ** (iii) if d = smallest difference between adjacent unique x values, a = d/5, 
  ** (iv) a = (1.1; 1.5; 2.5; 5) x d/5
* using the data with added noise to one selected variable, we refit an autoML object (aml_changed)
* we select the best performing models from both runs (i.e. the best performing models in the toy_aml and aml_changed objects)
* we print:
  ** the positive or negative change in the overall predictive utility of the model (AUC and AUCPR)
  ** the mean change in the variable
  ** the mean change in the predicted probability of default
  ** the min change in the predicted probability of default
  ** the max change in the predicted probability of default
  ** the number of class changes that have happened due to the change in the variable 
  ** the correlation coefficient between the change in the variable and the change in the predicted probability
  ** the regression coefficient from a fitted OLS (y = change in the prediction ~ x = change in the variable)
  ** the scatter plot with an added smooth (y = change in the prediction ~ x = change in the variable)
2) remove a certain variable 
* we choose a variable that we want to remove, and we refit an autoML object (aml_drop)
* we choose the best performing models from both runs (i.e. the best performing models in the toy_aml and aml_drop)
* we print 
  ** the positive or negative change in the overall predictive utility of the model
  ** the mean change in the predicted probability of default
  ** the min change in the predicted probability of default
  ** the max change in the predicted probability of default
  ** the number of class changes that have happened due to dropping the variable 


Call libraries and load data 
```{r,include=FALSE}
rm(list=ls())
libraries = c("readr", "corrplot", "data.table", "RColorBrewer", "tidyverse", "kernlab", "e1071", "MLmetrics", "stargazer", "dplyr", "purrr", "xtable", "base", "ggplot2", "DescTools","stylo", "igraph", "xgboost", "DALEX", "DALEXtra", "stats", "ranger", "ALEPlot", "party", "lime", "MASS", "iml", "ROCR", "rpart", "e1071", "caret","MLmetrics", "Matrix", "Boruta", "Metrics", "fastshap", "fscaret", "kknn", "knitr", "reshape", "ggridges", "shapper", "scales", "randomForest", "pastecs", "writexl", "corrr", "PerformanceAnalytics", "Hmisc", "networkD3", "emstreeR", "pdp", "pROC", "ggcorrplot", "corrplot", "randomForestExplainer", "logistf", "h2o", "h2o4gpu")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
set.seed(7)

# Load data
df <- readRDS("~/GitHub/IWA_xai/2022_repo/Loan Performance Use Case/data/df_post_data_explor.rds")
df = df[,-9]
df <- df %>% 
  mutate(loan_status = recode(loan_status, 
                              "0" = 0, 
                              "1" = 1))
df$loan_status = as.factor(df$loan_status)

```


In case you are already running a cluster (check the console and enter Y)
```{r}
#h2o.shutdown()
```


Activate h2o cluster
```{r}
h2o.init(nthreads = -1, max_mem_size = "16g")
```

Select a small toy data
```{r}
toy_df <- df[,1:11] %>% 
  group_by(loan_status) %>% 
  sample_frac(0.02) %>% 
  ungroup() #
toy_train <- toy_df %>% 
  group_by(loan_status) %>% 
  sample_frac(0.7) %>% 
  ungroup() # Use 70% data set for training model. 
toy_test <- dplyr::setdiff(toy_df, toy_train) # Use 30% data set for validation. 
y = "loan_status"
x <- setdiff(names(toy_train), y)
toy_test <- as.h2o(toy_test)
toy_train <- as.h2o(toy_train)
```


Train the autoML and obtain the best performing model within the obeject 
```{r}
toy_aml <- h2o.automl(x = x, y = y,
                      training_frame = toy_train,
                      max_models = 20,
                      project_name = "loan_toy",
                      seed = 7)
lb_toy <- toy_aml@leaderboard
print(lb_toy, n = nrow(lb_toy)) 
lb_toy_df <- as.data.table(lb_toy)

best_auc <- h2o.get_best_model(toy_aml, criterion = "auc")
pred_best_auc <- h2o.predict(best_auc, toy_test)
perf_best_auc <- h2o.performance(best_auc, toy_test)
```


```{r}
test = toy_test
train = toy_train
```


Source sens function (sens_pred.R)
```{r}
source('~/GitHub/IWA_xai/2022_repo/Loan Performance Use Case/Functions and checks/sens_pred.R')
```


Select a variable 
```{r}
variable = "loan_amnt"
```


Different checks:
Sensitivity 1:a = z/50; 
Sensitivity 2: a = 1.1 x z/50
Sensitivity 3: a = 1.5 x z/50
Sensitivity 4: a = 2.5 x z/50
Sensitivity 5: a = 5 x z/50
Sensitivity 6: d = smallest difference between adjacent unique x values, a = d/5, 
Sensitivity 7: a = 1.1 x d/5
Sensitivity 8: a = 1.5 x d/5
Sensitivity 9: a = 2.5 x d/5
Sensitivity 10: a = 5 x d/5


```{r}
# Sensitivity 1:a = z/50; 
sensitivity_1 = sens(test = test, train = train, variable = variable, factor = 1, amount = 0)
```

```{r}
# Sensitivity 2: a = 1.1 x z/50
sensitivity_2 = sens(test = test, train = train, variable = variable, factor = 1.1, amount = 0)
```

```{r}
# Sensitivity 3: a = 1.5 x z/50
sensitivity_3 = sens(test = test, train = train, variable = variable, factor = 1.5, amount = 0)
```


```{r}
# Sensitivity 4: a = 2.5 x z/50
sensitivity_4 = sens(test = test, train = train, variable = variable, factor = 2.5, amount = 0)
```


```{r}
# Sensitivity 5: a = 5 x z/50
sensitivity_5 = sens(test = test, train = train, variable = variable, factor = 5, amount = 0)
```

```{r}
# Sensitivity 6: d = smallest difference between adjacent unique x values, a = d/5, 
sensitivity_6 = sens(test = test, train = train, variable = variable, factor = 1, amount = NULL)
```


```{r}
# Sensitivity 7: d = smallest difference between adjacent unique x values, a = 1.1*d/5, 
sensitivity_7 = sens(test = test, train = train, variable = variable, factor = 1.1, amount = NULL)
```


```{r}
# Sensitivity 8: d = smallest difference between adjacent unique x values, a = 1.5*d/5, 
sensitivity_8 = sens(test = test, train = train, variable = variable, factor = 1.5, amount = NULL)
```


```{r}
# Sensitivity 9: d = smallest difference between adjacent unique x values, a = 2.5*d/5, 
sensitivity_9 = sens(test = test, train = train, variable = variable, factor = 2.5, amount = NULL)
```


```{r}
# Sensitivity 10: d = smallest difference between adjacent unique x values, a = 5*d/5, 
sensitivity_10 = sens(test = test, train = train, variable = variable, factor = 5, amount = NULL)
```



Sensitivities (Type 1) for all numeric columns 
```{r}
sensitivities_1 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_1[[i]] = sens(test = test, train = train, variable = cols[i], factor = 1, amount = 0)
}
```

Sensitivities (Type 2) for all numeric columns 
```{r}
sensitivities_2 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_2[[i]] = sens(test = test, train = train, variable = cols[i], factor = 1.1, amount = 0)
}
```


Sensitivities (Type 3) for all numeric columns 
```{r}
sensitivities_3 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_3[[i]] = sens(test = test, train = train, variable = cols[i], factor = 1.5, amount = 0)
}
```


Sensitivities (Type 4) for all numeric columns 
```{r}
sensitivities_4 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_4[[i]] = sens(test = test, train = train, variable = cols[i], factor = 2.5, amount = 0)
}
```

Sensitivities (Type 5) for all numeric columns 
```{r}
sensitivities_5 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_5[[i]] = sens(test = test, train = train, variable = cols[i], factor = 5, amount = 0)
}
```


Sensitivities (Type 6) for all numeric columns 
```{r}
sensitivities_6 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_6[[i]] = sens(test = test, train = train, variable = cols[i], factor = 1, amount = NULL)
}
```


Sensitivities (Type 7) for all numeric columns 
```{r}
sensitivities_7 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_7[[i]] = sens(test = test, train = train, variable = cols[i], factor = 1.1, amount = NULL)
}
```



Sensitivities (Type 8) for all numeric columns 
```{r}
sensitivities_8 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_8[[i]] = sens(test = test, train = train, variable = cols[i], factor = 1.5, amount = NULL)
}
```


Sensitivities (Type 9) for all numeric columns 
```{r}
sensitivities_9 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_9[[i]] = sens(test = test, train = train, variable = cols[i], factor = 2.5, amount = NULL)
}
```


Sensitivities (Type 10) for all numeric columns 
```{r}
sensitivities_10 <- list()
nums = unlist(lapply(toy_df, is.numeric))  
cols = colnames(toy_df[,nums])
for (i in 1:length(cols)){
  sensitivities_10[[i]] = sens(test = test, train = train, variable = cols[i], factor = 5, amount = NULL)
}
```


Smallest difference between adjacent unique x values
```{r}
#x = sort(toy_df$loan_amnt)
#z = max(x) - min(x)
#d = diff(xx <- unique(sort.int(round(x, 3 - floor(log10(z))))))
#d = min(d)
```

